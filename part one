def connect_SQL(host, db, user, password, port, sql, conn):
    '''
    读取数据库
    :param host: host
    :param db: dbname
    :param user: user
    :param password: pwd
    :param port: port
    :param sql: sql语句
    :param conn: connection
    :return: data
    '''
    conn = psycopg2.connect(host=host, dbname=db, user=user, password=password, port=port)
    data = psql.read_sql(sql, conn)

def write_SQL(username, password, host, port, db, name, schema, df, if_exists):
    '''
    储存数据至数据库
    :param username:user 
    :param password: pwd
    :param host: host
    :param port: port
    :param db: dbname
    :param name: name of sql table
    :param schema: Specify the schema. If None, use default schema
    :param df: DataFrame
    :param if_exists: {‘fail’, ‘replace’, ‘append’}, default ‘fail’
    '''
    engine = create_engine('postgresql+psycopg2://{}:{}@{}/{}'.format(user,password,host:port,dbname))
    conn = engine.connect()
    df.to_sql(name, schema, conn, if_exists=if_exists,index=False) #if_exists=replace
    # conn.execute(sql).fetchall() #储存进数据库再查询

def csv_read(file_path):
    '''
    读取csv文件
    :param file_path: 文件路径
    :return: df数据
    '''
    df = pd.read_csv(file_path)
    return df

def excel_read(filepath):
    '''
    读取excel文件
    :param filepath: 文件路径
    :return: df数据
    '''
    df = pd.read_excel(filepath)
    return df

#读取pickle文件
def pickle_read(filepath):
    '''
    读取pickle文件
    :param filepath: 文件路径
    :return: df数据
    '''
    df = pd.read_pickle(filepath)
    return df

#读取txt文件
def txt_read(filepath):
    '''
    读取txt文件
    :param filepath: 文件路径
    :return: df数据
    '''
    df = pd.read_csv(filepath, header=None)
    return df

#查看数据
#对所有变量属性等进行统计
def df_describe(df):
    return df.describe()
#缺失、类别判断情况统计
def df_info(df):
    return df.info()

#返回df前5行、后5行
def tail_head(df):
    return df.head(), df.tail()

# 删除

#删除多列数据
def del_multiple_col(df, col_names_list):
    df.drop(col_names_list, axis=1, inplace=True)
    return df

#删除常数列
def del_constant_col(df):
    col_names_list = df.columns
    for col in col_names_list:
        if len(set(df[col])) == 1:
            print("{} is a constant var,we del it".format(col))
            col_names_list.remove(col)
    return col_names_list

# 如何区分出object变量
def distinguish_object(df):
    object_var = []
    col_names_list = df.columns
    for col in col_names_list:
        if df[col].dtype == 'object':
            object_var.append(col)
    return object_var

#数值型变量统计
def num_missing_count(df, col):
    missing_count = np.sum(df[col].map(lambda col: int(np.isnan(col))))
    missing_pcnt =  missing_count * 1.0 / df.shape[0]
    print("The missing rate of {} is {}".format(col, missing_pcnt))
    return missing_count, missing_pcnt

# 删除threshold大于一定数值的变量
def del_num_var(df, num_list, threshold):
    threshold = threshold
    for col in num_list:
        missing_count, missing_pcnt = num_missing_count(df, col)
        if missing_pcnt > threshold:
            print("we del {} because it misses too much".format(col))
            num_list.remove(col)
    return num_list

def cate_missing_count(df, col):
    missing_count = np.sum(df[col].map(lambda col: int(col!=col)))
    missing_pcnt = missing_count * 1.0 / df.shape[0]
    print("The missing rate of {} is {}".format(col, missing_pcnt))
    return missing_count, missing_pcnt

def del_cate_var(df, cate_list, threshold):
    threshold = threshold
    for col in cate_list:
        missing_count, missing_pcnt = cate_missing_count(df, col)
        if missing_pcnt > threshold:
            print("we del {} because it misses too much".format(col))
            cate_list.remove(col)
    return cate_list

#单变量统计

# 变量中各类别出现的频次、频率
# 统计变量中不同标签的数量
def label_uniq_cnt(df, col):
    label_uniq_cnt = {}
    for label in df[col]:
        if label not in label_uniq_cnt:
            label_uniq_cnt[label] = 0
        label_uniq_cnt[label] = label_uniq_cnt[label] + 1
    return label_uniq_cnt

# 相关属性的统计
def single_stats(df, col):
    return pd.Series([df[col].min(), df[col].max(), df[col].mode, df[col].quantile(.25), df[col].quantile(.75),
                     df[col].quantile(.95), df[col].quantile(.99), df[col].median(), df[col].std(), df[col].var()],
    index=["最小值", "最大值", "众数", "1/4分位数", "3/4分位数", "95分位数", "99分位数", "中位数", "标准差", "方差"])

#极端值发现
def abormal_num_found(df, col):
    median_cate = df[col].median()
    std_cate = df[col].std()
    upper_limit = median_cate - 3 * std_cate
    lower_limit = median_cate + 3 * std_cate
    '''
    one_quarter = df[col].quantile(.25)
    three_quarter = df[col].quantile(.75)
    irq = three_quarter - one_quarter
    upper_limit = one_quarter - 1.5 * irq
    lower_limit = three_quarter + 1.5 * irq
    '''
    abnormal_upper = df[col] > upper_limit
    abnormal_lower = df[col] < lower_limit
    df[col].ix[df[col]>upper_limit] = upper_limit
    df[col].ix[df[col]<lower_limit] = lower_limit
    print("{} 中有 {} 个异常值".format(col, str(abnormal_upper.sum() + abnormal_lower.sum())))
    return df

#计算自己需要列之间相关矩阵
def corr_cate(df, columns):
    data = df[columns]
    corr = data.corr()
    return corr

# 计算某一列与其他多列的相关系数
def corr_cate1(df, col, column_list): # col属于columns_list
    data = df[column_list]
    corr = data.corr()[col]
    return corr

# 计算某一列与某一列的相关系数
def corr_cate2(df, col1, col2):
    corr = data[col1].corr(data[col2])
    return corr

# 随机抽样
def single_random_sampling(df, n): # n: 抽取的数量或者比例
    data = df.sample(n)  # random.sample(df, n)
    return data

def stratified_sampling(df, col, n_sample): # 1:1抽样
    n = min(n_sample, df[col].value_counts.min())
    data = df.groupby(col).apply(lambda x:x.sample(n))
    data.index = data.index.droplevel(0)
    return data

#不同比例抽样
gbr = data.groupby(col)
print(gbr.groups)
frac_dict={
    0: 0.2,
    1: 0.8
}
def typeicalSampling(group, typeicalFracDict):
    name = group.name
    frac = typeicalFracDict[name]
    return group.sample(frac=frac)
result=df.groupby(label,group_keys=False).apply(typeicalSampling,typeicalFracDict)

#产生训练集、测试集
def split_data(data, label):
    y = data[label]
    X = data[~label]
    X_train, y_train, X_test, y_test = train_test_split(X, y, test_size=0.2, random_state=123456)
    return X_train, y_train, X_test, y_test

##交叉验证
def cross_validate(data, label, n_split):
    y = data[label]
    X = data[~label]
    sss = StratifiedShuffleSplit(n_splits=n_split, test_size=0.2)
    for train_index, test_index in sss.split(X, y):
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = labels[train_index], labels[test_index]
    return X_train, y_train, X_test, y_test

#拼接
def merge_link(df1, df2, key1, key2, bool1, bool2, how):
    '''
    how='outer':全连接，并集
    how='left':左连接，左边取全部，右边取部分，没有值则用NaN填充
    how='right':右连接，右边取全部，左边取部分，没有值则用NaN填充
    left_on,right_on:如果左右侧DataFrame的连接键列名不一致，但是取值有重叠，可使用left_on、right_on来指定左右连接键
    left_index,right_index:索引作为连接键
    '''
    data = pd.merge(df1, df2, left_on=key1, right_on=key2, left_index=bool1, right_index=bool2, how=how)
    return data

def join_link(df1, df2, how):
	data = pd.merge(df1, df2, how=how)
	return data

def concat_link(df1, df2, axis, join):
	'''
	axis: 0(default) or 1
	join: 类似于merge中的how
	'''
	data = pd.concat(df1, df2, axis=axis, join=join)
	return data

'''
	填补缺失值几种方式：
		1.直接删除,若缺失比例较大时，分布会改变
		2.标记缺失值，缺失值标记为1，不缺失标记为0
		3.用一个值去替换缺失值,数值型变量用均值，类别型变量用众数
		4.随机抽样法填补缺失值
		5.用上下数据进行填充
		6.用插值法进行填充
		7.算法拟合进行填充
		8.高维映射
		9.不处理
		10.K-means方法
'''
def fill_na1(df, missing_col_list):
    list_symbol = [',', '.', '!', '@', '$', '#', '%', '^',
                   '&', '*', '(', ')', '/', '<', '>']
    for col in missing_col_list:
        for x in df[col]:
            if str(x).isspace():
                x = np.nan
            if x in list_symbol:
                x = np.nan
    data = df.dropna()
    return data

def fill_na2(df, col):
    df["是否缺失"] = 0
    df.loc[df[col].isnull(), "是否缺失"] = 1
    return df

def fill_na3(df, col, method):
    col_mean = df[col].mean()
    col_median = df[col].median()
    col_mode = df[col].mode()
    if method == mean:
        df[col].fillna(col_mean, inplace=True)
    elif method == median:
        df[col].fillna(col_median, inplace=True)
    elif method == mode:
        df[col].fillna(col_mode, inplace=True)
    elif method == pad:
        df[col].fillna(method="pad", inplace=True)
    elif method == bfill:
        df[col].fillna(method="bfill", inplace=True)
    return df

def random_make_up(df, col):
    not_missing_df = df.loc[df[col].notnull, col]
    missing_df_index = df.loc[df[col].isnull(), col].index
    make_up_sample = random.sample(list(not_missing_df), len(missing_df_index))
    df.loc[missing_df_index, col] = make_up_sample
    return df

def interpolate(df, col):
    return df[col].interpolate()

# 无量纲化
# one-hot编码， getdummy(处理类别变量)
def one_hot(df, col):
    one_hot_encode = preprocessing.OneHotEncoding()
    one_hot_encoded = one_hot_encode.fit_transform(df[col])
    return df

def get_dummy(df, col):
    data = pd.get_dummies(df[col], prefix=col)
    return data

#标准化
def scale(df, col):
    data = preprocessing.StandardScaler().fit_transform(df[col])
    return data

'''
分箱操作：自定义分箱、等频分箱、等距分箱、聚类、卡方分箱
'''

#等频分箱,并计算IV、WOE值
def bin_frequence(X, y, n):
    total = y.count()
    bad = y.sum()
    good = total - bad
    d1 = pd.DataFrame({"X":X, "Y":y, "bucket":pd.qcut(X, n)})
    d2 = d1.groupby("bucket", as_index=True) #按照分箱的结果进行聚合
    d3 = pd.DataFrame(d2.X.min(),columns=["min_bin"])
    d3["min_bin"] = d2.X.min() #箱体左边界
    d3["max_bin"] = d2.X.max() #箱体右边界
    d3["bad"] = d2.y.sum()
    d3["total"] = d2.y.count()
    d3["bad_rate"] = d3["bad"] / d3["total"]
    d3["bad_attribute"] = d3["bad"] / bad
    d3["good_attribute"] = (d3["total"] - d3["bad"]) / good
    d3["WOE"] = np.log(d3["good_attribute"] / d3["bad_attribute"])
    d3["IV"] = (d3["good_attribute"] - d3["bad_attribute"]) * d3["WOE"]
    IV = ((d3["good_attribute"] - d3["bad_attribute"]) * d3["WOE"]).sum()
    d4 = (d3.sort_values(by="min_bin")).reset_index(drop=True)
    print("分箱结果：")
    print(d4)
    print("IV值：")
    print(IV)
    cut = []
    cut.append(float("-inf"))
    for i in d4.min_bin:
        cut.append(i)
    cut.append(float("inf"))
    return d4, IV, cut

#等距分箱
def bin_distance(X, y, n):
    total = y.count()
    bad = y.sum()
    good = total - bad
    d1 = pd.DataFrame({"X":X, "Y":y, "bucket":pd.cut(X, n)})
    d2 = d1.groupby("bucket", as_index=True)
    d3 = pd.DataFrame(d2.X.min(), columns=["min_bin"])
    d3["min_bin"] = d2.X.min()
    d3["max_bin"] = d2.X.max()
    d3["total"] = d2.y.count()
    d3["bad"] = d2.y.sum()
    d3["bad_rate"] = d3["bad"] / d3["total"]
    d3["bad_attribute"] = d3["bad"] / bad
    d3["good_attribute"] = (d3["total"] - d3["bad"]) / good
    d3["WOE"] = np.log(d3["good_attribute"] / d3["bad_attribute"])
    d3["IV"] = (d3["good_attribute"] - d3["bad_attribute"]) * d3["WOE"]
    IV = ((d3["good_attribute"] - d3["bad_attribute"]) * d3["WOE"]).sum()
    d4 = (d3.sort_values(by="min_bin")).reset_index(drop=True) #reset_index:还原索引
    print("IV值为：")
    print(IV)
    print("分箱为：")
    print(d4)
    cut = []
    cut.append(float("-inf"))
    for i in d4["min_bin"]:
        cut.append(i)
    cut.append(float("inf"))
    return d4, IV, cut

#自定义分箱
def bin_distance(X, y, bins):
    total = y.count()
    bad = y.sum()
    good = total - bad
    d1 = pd.DataFrame({"X":X, "Y":y, "bucket":pd.cut(X, bins)})
    d2 = d1.groupby("bucket", as_index=True)
    d3 = pd.DataFrame(d2.X.min(), columns=["min_bin"])
    d3["min_bin"] = d2.X.min()
    d3["max_bin"] = d2.X.max()
    d3["total"] = d2.y.count()
    d3["bad"] = d2.y.sum()
    d3["bad_rate"] = d3["bad"] / d3["total"]
    d3["bad_attribute"] = d3["bad"] / bad
    d3["good_attribute"] = (d3["total"] - d3["bad"]) / good
    d3["WOE"] = np.log(d3["good_attribute"] / d3["bad_attribute"])
    d3["IV"] = (d3["good_attribute"] - d3["bad_attribute"]) * d3["WOE"]
    IV = ((d3["good_attribute"] - d3["bad_attribute"]) * d3["WOE"]).sum()
    d4 = (d3.sort_values(by="min_bin")).reset_index(drop=True) #reset_index:还原索引
    print("IV值为：")
    print(IV)
    print("分箱为：")
    print(d4)
    cut = []
    cut.append(float("-inf"))
    for i in d4["min_bin"]:
        cut.append(i)
    cut.append(float("inf"))
    return d4, IV, cut

#卡方分箱
def ChiMegre(df, col, target, confidence=3.841, bins=10, samples=None): #confidence,置信度阈值为3.841，自由度为1
    if samples != None:
        df = random.sample(df, n=samples)
    else:
        df

    total = df.groupby(col)[target].count()
    total = pd.DataFrame({"total":total})
    bad = df.groupby(col)[target].sum()
    bad = pd.DataFrame({"bad":bad})
    regroup = total.merge(bad, left_index=True, right_index=True, how="inner")
    regroup.reset_index(inplace=True)
    regroup["good"] = regroup["total"] - regroup["bad"]
    regroup = regroup.drop("total", axis=1) #删除total列
    regroup = np.array(regroup) #转化为numpy,提高计算效率 #col, bad, good
    print("已经完成数据读入，正在计算数据初处理")
    #处理连续没有正样本或者负样本的区间，并进行区间合并（以免卡方值计算报错）
    i = 0
    while (i <= regroup.shape[0] - 2):
        if ((regroup[i, 1]==0 and regroup[i+1, 1]==0) or (regroup[i, 2]==0 and regroup[i+1, 2]==0)):
            regroup[i,1] = regroup[i,1] + regroup[i+1,1] # 负样本
            regroup[i,2] = regroup[i,2] + regroup[i+1,2] # 正样本
            regroup[i,0] = regroup[i,0] + regroup[i+1,0] # col
            regroup = np.delete(regroup, i+1, 0)
            i = i - 1
        i = i + 1
    #相邻两个区间进行卡方值计算
    chi_table = np.array([]) #创建一个数组保存相邻两个区间的卡方值
    for i in np.arange(regroup.shape[0]-1):
        chi = (regroup[i,1]*regroup[i+1,2]-regroup[i,2]*regroup[i+1,1])**2 \
            *(regroup[i,1]+regroup[i,2]+regroup[i+1,1]+regroup[i+1,2])/ \
                ((regroup[i,1]+regroup[i,2])*(regroup[i+1,1]+regroup[i+1,2])* \
                (regroup[i,1]+regroup[i+1,1])*(regroup[i,2]+regroup[i+1,2]))
        chi_table = np.append(chi_table,chi)
    print("已完成数据初处理，正在进行卡方分箱核心操作")
    #把卡方值最小的两个区间进行合并（卡方分箱合并）
    while (1):
        if (len(chi_table) <= (bins - 1) and min(chi_table) > confidence):
            break
        chi_min_index = np.argwhere(chi_table==min(chi_table))[0] # 取出最小卡方值的索引
        regroup[chi_min_index,1] = regroup[chi_min_index,1] + regroup[chi_min_index+1,1]
        regroup[chi_min_index,2] = regroup[chi_min_index,2] + regroup[chi_min_index+1,2]
        regroup[chi_min_index,0] = regroup[chi_min_index+1,0]
        regroup = np.delete(regroup, chi_min_index+1, 0)

        if (chi_min_index == regroup.shape[0]-1): #尝试最后两个区间
            # 计算合并后当前区域与前一个区域的卡方值并合并
            chi_table[chi_min_index-1] = (regroup[chi_min_index - 1, 1] * regroup[chi_min_index, 2] - regroup[chi_min_index - 1, 2] * regroup[chi_min_index, 1]) ** 2 \
                    * (regroup[chi_min_index - 1, 1] + regroup[chi_min_index - 1, 2] + regroup[chi_min_index, 1] + regroup[chi_min_index, 2]) / \
                    ((regroup[chi_min_index - 1, 1] + regroup[chi_min_index - 1, 2]) * (regroup[chi_min_index, 1] + regroup[chi_min_index, 2]) * (regroup[chi_min_index - 1, 1] + regroup[chi_min_index, 1]) * (regroup[chi_min_index - 1, 2] + regroup[chi_min_index, 2]))
            chi_table = np.delete(chi_table,chi_min_index,0)
        else:
            chi_table[chi_min_index - 1] = (regroup[chi_min_index - 1, 1] * regroup[chi_min_index, 2] -regroup[chi_min_index - 1, 2] * regroup[chi_min_index, 1]) ** 2 \
                    * (regroup[chi_min_index - 1, 1] + regroup[chi_min_index - 1, 2] +regroup[chi_min_index, 1] + regroup[chi_min_index, 2]) / \
                    ((regroup[chi_min_index - 1, 1] + regroup[chi_min_index - 1, 2]) * (regroup[chi_min_index, 1] + regroup[chi_min_index, 2]) * (regroup[chi_min_index - 1, 1] + regroup[chi_min_index, 1]) * (regroup[chi_min_index - 1, 2] + regroup[chi_min_index, 2]))

            chi_table[chi_min_index] = (regroup[chi_min_index, 1] * regroup[chi_min_index+1, 2] - regroup[chi_min_index, 2] * regroup[chi_min_index+1, 1]) ** 2 \
                    * (regroup[chi_min_index, 1] + regroup[chi_min_index, 2] + regroup[chi_min_index+1, 1] + regroup[chi_min_index+1, 2]) / \
                    ((regroup[chi_min_index, 1] + regroup[chi_min_index, 2]) * (regroup[chi_min_index+1, 1] + regroup[chi_min_index+1, 2]) * (regroup[chi_min_index, 1] + regroup[chi_min_index+1, 1]) * (regroup[chi_min_index, 2] + regroup[chi_min_index+1, 2]))
            chi_table = np.delete(chi_table, chi_min_index+1,axis=0)
    print("已完成卡方核心分箱，正在保存结果")

    # 保存结果成一个数据框
    result = pd.DataFrame()
    result[col] = [col] * regroup.shape[0] #结果表第一列：变量名称
    list_tmp = []
    for i in np.arange(regroup.shape[0]):
        if i == 0:
            x = '0' + ','+ str(regroup[i,0])
        elif i == regroup.shape[0]-1:
            x = str(regroup[i-1,0]) + "+"
        else:
            x = str(regroup[i-1,0]) + ","+ str(regroup[i,0])
        list_tmp.append(x)
    result["interval"] = list_tmp
    result["bad"] = regroup[:,1]
    result["good"] = regroup[:,2]
    result["bad_rate"] = result["bad"] / total
    result["bad_attribute"] = result["bad"] / bad
    result["good_attribute"] = result["good"] / (total - bad)
    result["WOE"] = np.log(result["good_attribute"] / result["bad_attribute"])
    result["IV"] = (result["good_attribute"] - result["bad_attribute"]) * result["WOE"]
    return result

# Filter筛选
#方差筛选
from sklearn.feature_selection import VarianceThreshold
#方差选择法，返回值为特征选择后的数据，参数threshold为方差的阈值
VarianceThreshold(threshold=3).fit_transform(iris.data)

from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
#选择K个最好的特征，返回选择特征后的数据
SelectKBest(chi2, k=2).fit_transform(iris.data, iris.target)

from sklearn.feature_selection import SelectKBest
from minepy import MINE

#由于MINE的设计不是函数式的，定义mic方法将其为函数式的，返回一个二元组，二元组的第2项设置成固定的P值0.5
def mic(x, y):
    m = MINE()
    m.compute_score(x, y)
    return (m.mic(), 0.5)
#选择K个最好的特征，返回特征选择后的数据
SelectKBest(lambda X, Y: array(map(lambda x:mic(x, Y), X.T)).T, k=2).fit_transform(iris.data, iris.target)

#递归特征消除法
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression
#递归特征消除法，返回特征选择后的数据，参数estimator为基模型，参数n_features_to_select为选择的特征个数
RFE(estimator=LogisticRegression(), n_features_to_select=2).fit_transform(iris.data, iris.target)
